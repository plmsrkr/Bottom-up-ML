{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet as wnimport\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('glove.6B.100d 2.txt', 'r') \n",
    "vocab = []\n",
    "embeddings = {}\n",
    "matrix = []\n",
    "for line in f:\n",
    "    split_line = line.split()\n",
    "    word = split_line[0]\n",
    "    vector = np.asarray([float(i) for i in split_line[1:]])\n",
    "    embeddings.update({word:vector})\n",
    "    vocab.append(word)\n",
    "    matrix.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(word, relation, k = 4, return_list = False):\n",
    "    \"\"\"Takes in a target word, a list of two words expressing a relation,\n",
    "    an integer denoting the amount of values that should be returned, and whether a list of words should be returned,\n",
    "    or only the first item. k is set to 4 by default, in the case that the target word and the relation words\n",
    "    answer the analogy.\n",
    "    Returns a list of words of length [k - 3, k] that satisfy the analogy in order of decreasing cosine similarity\"\"\"\n",
    "    if [i for i in [word, relation[0], relation[1]] if i not in embeddings]:\n",
    "        raise ValueError(\"Word must be in vocabulary\")\n",
    "    result_vector = embeddings[relation[0]] - embeddings[relation[1]] + embeddings[word]\n",
    "    nearest_indices = k_nearest_vectors(k, matrix, [result_vector])[0]\n",
    "    closest_words = [vocab[i] for i in nearest_indices if vocab[i] != word and vocab[i] not in relation]\n",
    "    if return_list:\n",
    "        return closest_words\n",
    "    else:\n",
    "        return closest_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_vectors(k, mtx, candidate_vector):\n",
    "    \"\"\"Takes in an integer value(k), a matrix (2D list) of all the vectors, and the vector (list) we want to compare.\n",
    "    Returns an array of length k for indices of the most similar word vectors, and the cosine similarities of these vectors \"\"\"\n",
    "    cos_similarities = cosine_similarity(mtx, candidate_vector).flatten()\n",
    "    k_sorted = np.flip(np.argsort(cos_similarities)[-k:], axis = 0)\n",
    "    cos_sorted = np.flip(np.sort(cos_similarities), axis = 0)[:k]\n",
    "    return k_sorted, cos_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('king', ['woman', 'man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereotype and Bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('nurse', ['man', 'doctor']) #gender based on occupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literally says Nurse to Doc as woman is to man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'brown'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('criminal', ['white', 'police']) #racial stereotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'islamic'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('terrorist', ['christianity', 'lawful']) #religious stereotypes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compare cosine similarities\n",
    "def cosine_sim(target,relation):\n",
    "    \n",
    "      return({target:[{relation[0]:cosine_similarity([embeddings[target]],[embeddings[relation[0]]])[0].tolist()},{relation[1]:cosine_similarity([embeddings[target]],[embeddings[relation[1]]])[0].tolist()}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'engineer': [{'man': [0.4299850925969518]}, {'woman': [0.3340311091827807]}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim('engineer', ['man','woman'])# stereotype in profession\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'engineer': [{'african': [0.22085807332344548]},\n",
       "  {'american': [0.42412603855330006]}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim('engineer', ['african','american'])# stereotype in profession by location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'terrorist': [{'christianity': [0.19160212301043505]},\n",
       "  {'islam': [0.4048855348671819]}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim('terrorist', ['christianity', 'islam'])# religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beauty': [{'black': [0.42221943915798205]}, {'white': [0.3061925933575558]}]}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim('beauty', ['black', 'white']) # in races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neutralize bias for non-gender specific words: I'll use an algorithm by Boliukbasi et al., 2016 to perform gender debiasing. Note that some word pairs such as \"actor\"/\"actress\" or \"grandmother\"/\"grandfather\" should remain gender specific, while other words such as \"receptionist\" or \"technology\" should be neutralized, i.e. not be gender-related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis.\n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    Arguments:\n",
    "        word -- string indicating the word to debias\n",
    "        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
    "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
    "    Returns:\n",
    "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Select word vector representation of \"word\"\n",
    "    e = embeddings[word]\n",
    "\n",
    "    # Compute e_biascomponent\n",
    "    e_biascomponent = np.divide(np.dot(e, g), np.linalg.norm(g) ** 2) * g\n",
    "\n",
    "    # Neutralize e by substracting e_biascomponent from it\n",
    "    e_debiased = e - e_biascomponent\n",
    "\n",
    "    return e_debiased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender concept from GloVe vectors\n",
    "g = embeddings['woman'] - embeddings['man']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18769064]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([embeddings['man']], [g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.388177]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([embeddings['woman']], [g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- compared to the gender subspace the word 'man' is -ve and 'woman' is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine similarity between a given word and the gender concept\n",
      "\n",
      "lipstick [[0.18037245]]\n",
      "guns [[-0.09964446]]\n",
      "science [[-0.02147577]]\n",
      "arts [[0.01484675]]\n",
      "literature [[0.08261854]]\n",
      "warrior [[-0.156342]]\n",
      "doctor [[0.10942282]]\n",
      "tree [[-0.0886836]]\n",
      "receptionist [[0.28068759]]\n",
      "technology [[-0.14474527]]\n",
      "fashion [[0.08097437]]\n",
      "teacher [[0.15233696]]\n",
      "engineer [[-0.12300012]]\n",
      "pilot [[-0.04113394]]\n",
      "computer [[-0.11545715]]\n",
      "singer [[0.11372643]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCosine similarity between a given word and the gender concept\\n\")\n",
    "word_list = ['lipstick', 'guns', 'science', 'arts', 'literature', 'warrior','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'pilot', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity([embeddings[w]], [g]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we see that words like lipstick and singer are closer to women and guns are closer to men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cosine similarity between receptionist and g, before neutralizing:  [[0.28068759]]\n",
      "\n",
      "cosine similarity between receptionist and man, before neutralizing:  [[0.18974931]]\n",
      "\n",
      "cosine similarity between receptionist and woman, before neutralizing:  [[0.33642033]]\n"
     ]
    }
   ],
   "source": [
    "# before neutralizing\n",
    "word='receptionist'\n",
    "print(\"\\ncosine similarity between \" + word + \" and g, before neutralizing: \", cosine_similarity([embeddings[\"receptionist\"]], [g]))\n",
    "print(\"\\ncosine similarity between \" + word + \" and man, before neutralizing: \", cosine_similarity([embeddings[\"receptionist\"]], [embeddings[\"man\"]]))\n",
    "print(\"\\ncosine similarity between \" + word + \" and woman, before neutralizing: \", cosine_similarity([embeddings[\"receptionist\"]], [embeddings[\"woman\"]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cosine similarity between receptionist and g, after neutralizing:  [[6.9388939e-18]]\n",
      "\n",
      "cosine similarity between receptionist and g, after neutralizing:  [[0.2525859]]\n",
      "\n",
      "cosine similarity between receptionist and g, after neutralizing:  [[0.2525859]]\n"
     ]
    }
   ],
   "source": [
    "# after neutralizing the distances are equal\n",
    "n_word=neutralize('receptionist',g,embeddings)\n",
    "print(\"\\ncosine similarity between \" + word + \" and g, after neutralizing: \", cosine_similarity([n_word], [g]))\n",
    "print(\"\\ncosine similarity between \" + word + \" and g, after neutralizing: \", cosine_similarity([n_word], [embeddings[\"man\"]]))\n",
    "print(\"\\ncosine similarity between \" + word + \" and g, after neutralizing: \", cosine_similarity([n_word], [embeddings[\"man\"]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cosine similarity between engineer and g, before neutralizing:  [[-0.12300012]]\n"
     ]
    }
   ],
   "source": [
    "word='engineer'\n",
    "print(\"\\ncosine similarity between \" + word + \" and g, before neutralizing: \", cosine_similarity([embeddings[\"engineer\"]], [g]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cosine similarity between receptionist and g, after neutralizing:  [[-3.46944695e-17]]\n"
     ]
    }
   ],
   "source": [
    "n_word=neutralize('engineer',g,embeddings)\n",
    "print(\"\\ncosine similarity between \" + word + \" and g, after neutralizing: \", cosine_similarity([n_word], [g]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distances are almost similar now\n",
      "\n",
      "cosine similarity between engineer and woman, after neutralizing:  [[0.38469807]]\n",
      "\n",
      "cosine similarity between engineer and man, after neutralizing:  [[0.41001249]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The distances are almost similar now\")\n",
    "print(\"\\ncosine similarity between \" + word + \" and woman, after neutralizing: \", cosine_similarity([n_word], [embeddings[\"woman\"]]))\n",
    "print(\"\\ncosine similarity between \" + word + \" and man, after neutralizing: \", cosine_similarity([n_word], [embeddings[\"man\"]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equalization \n",
    "- algorithm for gender-specific words: Equalization is applied to pairs of words that you might want to have differ only through the gender property. As a concrete example, suppose that \"actress\" is closer to \"babysit\" than \"actor.\" By applying neutralizing to \"babysit\" we can reduce the gender-stereotype associated with babysitting. But this still does not guarantee that \"actor\" and \"actress\" are equidistant from \"babysit.\" The equalization algorithm takes care of this. The key idea behind equalization is to make sure that a particular pair of words are equidistant from the gender concept encoded by the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Debias gender specific words by following the equalize method\n",
    "    Arguments:\n",
    "    pair -- pair of strings of gender specific words to debias, e.g. (\"actress\", \"actor\")\n",
    "    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender\n",
    "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
    "    Returns\n",
    "    e_1 -- word vector corresponding to the first word\n",
    "    e_2 -- word vector corresponding to the second word\n",
    "    \"\"\"\n",
    "\n",
    "    # Select word vector representation of \"word\"\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "\n",
    "    # Compute the mean of e_w1 and e_w2\n",
    "    mu = (e_w1 + e_w2) / 2.0\n",
    "\n",
    "    # Compute the projections of mu over the bias axis and the orthogonal axis\n",
    "    mu_B = np.divide(np.dot(mu, bias_axis), np.linalg.norm(bias_axis) ** 2) * bias_axis\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Apply the formula\n",
    "    e_w1B = np.divide(np.dot(e_w1, bias_axis), np.linalg.norm(bias_axis) ** 2) * bias_axis\n",
    "    e_w2B = np.divide(np.dot(e_w2, bias_axis), np.linalg.norm(bias_axis) ** 2) * bias_axis\n",
    "    corrected_e_w1B = np.sqrt(np.abs(1 - np.sum(mu_orth ** 2))) * np.divide(e_w1B - mu_B, np.abs(e_w1 - mu_orth - mu_B))\n",
    "    corrected_e_w2B = np.sqrt(np.abs(1 - np.sum(mu_orth ** 2))) * np.divide(e_w2B - mu_B, np.abs(e_w2 - mu_orth - mu_B))\n",
    "\n",
    "    # Debias by equalizing e1 and e2 to the sum of their corrected projections\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "\n",
    "    return e1, e2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cosine similarities before equalizing:\n",
      "\n",
      "cosine_similarity(word_to_vec_map[\"actor\"], gender) =  [[0.4050496]]\n",
      "cosine_similarity(word_to_vec_map[\"actress\"], gender) =  [[0.26930187]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\ncosine similarities before equalizing:\")\n",
    "print(\"\\ncosine_similarity(word_to_vec_map[\\\"actor\\\"], gender) = \", cosine_similarity([embeddings[\"actor\"]], [embeddings['engineer']]))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"actress\\\"], gender) = \", cosine_similarity([embeddings[\"actress\"]], [embeddings['engineer']]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities after equalizing:\n",
      "\n",
      "cosine_similarity(e1, gender) =  [[0.16561745]]\n",
      "cosine_similarity(e2, gender) =  [[-0.10535123]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e1, e2 = equalize((\"actor\", \"actress\"), g, embeddings)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"\\ncosine_similarity(e1, gender) = \", cosine_similarity([e1], [embeddings['engineer']]))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity([e2], [embeddings['engineer']]))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although the vectors are not exactly equidistant but they are almost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91288564, 0.5180095 ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "pca = PCA(n_components = 2)\n",
    "mtx_2d = pca.fit_transform(matrix)\n",
    "embeddings_2d = {vocab[i]:mtx_2d[i] for i in np.arange(len(vocab))}\n",
    "#Our x-coordinate explains a little over 90% of the variance, and the y-coordinate explains a little over half.\n",
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def plot_embeddings(coords, embedding_dict, dot_color, labels = True):\n",
    "    \n",
    "    extract_coords = lambda tuple_index: [c[tuple_index] for c in coords] \n",
    "    x = extract_coords(0)\n",
    "    y = extract_coords(1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x, y, color = dot_color)\n",
    "    if labels:\n",
    "        for i in np.arange(len(word_lst)):\n",
    "            ax.annotate(word_lst[i], (x[i] + 0.05, y[i] + 0.05))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-9f832834ae78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_lst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'engineer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membedding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_lst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgender_ax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#base: man-woman, true: queen - king, false: engineer - housewife\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgender_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-9f832834ae78>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_lst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'engineer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membedding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_lst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgender_ax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#base: man-woman, true: queen - king, false: engineer - housewife\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgender_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_dict' is not defined"
     ]
    }
   ],
   "source": [
    "word_lst=['actor', 'actress', 'engineer']\n",
    "coords = [embedding_dict[word] for word in word_lst]\n",
    "gender_ax = plot_embeddings(coords,embeddings, embeddings_2d, 'blue') \n",
    "#base: man-woman, true: queen - king, false: engineer - housewife\n",
    "gender_ax.set_xlim(right=5)\n",
    "gender_ax.set_ylim(top = 1.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-dea7ec5e4144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgender_ax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'engineer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#base: man-woman, true: queen - king, false: engineer - housewife\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgender_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgender_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.55\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-19c9c0c4130c>\u001b[0m in \u001b[0;36mplot_embeddings\u001b[0;34m(word_lst, embedding_dict, dot_color, labels)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membedding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_lst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mextract_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtuple_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-19c9c0c4130c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_lst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membedding_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_lst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mextract_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtuple_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_coords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "gender_ax = plot_embeddings([e1, e2, 'engineer'], embeddings_2d, 'blue') \n",
    "#base: man-woman, true: queen - king, false: engineer - housewife\n",
    "gender_ax.set_xlim(right=5)\n",
    "gender_ax.set_ylim(top = 1.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
